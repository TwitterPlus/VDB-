# Vector Embeddings + RAG Playground

This repo is a **learning playground** for vector embeddings, RAG (Retrieval-Augmented Generation), and LLMs.

It is deliberately designed as a **blend of vibe coding and prompt engineering**:
- You will see practical, end-to-end code that "just works" for concrete use cases.
- You are encouraged to tweak prompts, change models, swap in different documents, and experiment with the retrieval pipeline.

## What this project does

This project shows how to build a small RAG system over **local text documents**. By default, the repo ships with a note from our lecturer Mr. Harry (`Demystifying The Myth.txt`).

Core components:
- **Embeddings**: `sentence-transformers` with the `all-MiniLM-L6-v2` model to embed text chunks.
- **Vector store**: `ChromaDB` to store and retrieve chunk-level embeddings.
- **LLM**: Google Gemini (configured via API key in `.env`, using `google-generativeai`).
- **Interface**: A Streamlit chatbot (`app.py`) that lets you ask natural language questions and get answers as if chatting with a normal assistant.

## Files and structure

- `app.py`  
  Streamlit app exposing a chatbot UI. Under the hood it:
  - Loads your Gemini API key from `.env`.
  - Ensures a local Chroma collection is populated with embedded text chunks from your documents.
  - Embeds your questions, retrieves relevant chunks, and asks Gemini to answer using only that context.

- `test_chromadb.ipynb`  
  Jupyter notebook to explore the pieces step-by-step:
  - Creating a Chroma collection.
  - Loading and chunking `Demystifying The Myth.txt` into paragraphs.
  - Embedding with `sentence-transformers`.
  - Running similarity search queries.

- `Demystifying The Myth.txt`  
  A note from Mr. Harry Atieku-Boateng talking about AI and intelligence.

- `requirements.txt`  
  Python dependencies for the virtual environment.

- `.env` (ignored by git)  
  Stores your **Gemini API key** locally (see below). This file is **not** committed to version control.

- `chroma_db/` (ignored by git)  
  Local on-disk storage for Chroma. It is a cache/index, not source code.

## Setup instructions

### 1. Python and virtual environment

From the project root:

```bash
python -m venv .venv
# Windows PowerShell
.venv\Scripts\Activate.ps1

# or cmd
.venv\Scripts\activate.bat
```

Then install dependencies:

```bash
pip install -r requirements.txt
```

### 2. Configure Gemini API key

Create a file named `.env` in the project root (same folder as `app.py`) with:

```env
GEMINI_API_KEY=your_real_api_key_here
```

This repo includes a `.gitignore` entry so `.env` is never committed to git.

### 3. Run the Streamlit chatbot

From the project root, with the virtual environment activated:

```bash
streamlit run app.py
```

Then open the local URL that Streamlit prints in the terminal (usually `http://localhost:8501`).

You should see a chatbot UI where you can:
- Ask questions about the content of your documents (for example: *"What is AI?"* if you keep the default policies file).
- Receive answers generated by Gemini, grounded in the retrieved text.

Behind the scenes, for each question, the app:
1. Embeds the question.
2. Retrieves the top-k most relevant text chunks from Chroma.
3. Builds a prompt with those chunks as context.
4. Calls Gemini to generate an answer.

## For students: how to learn from this repo

This repo is intended as a **hands-on lab** for:

- **Vector embeddings**  
  Learn how raw text gets mapped into vectors and why cosine similarity / nearest neighbors matter.

- **Retrieval-Augmented Generation (RAG)**  
  See how separating retrieval (Chroma + embeddings) from generation (LLM) makes systems more controllable and data-aware.

- **Prompt engineering**  
  Open `app.py` and look at how the prompt is constructed in `generate_answer`. Try:
  - Making the assistant more strict or more conversational.
  - Changing instructions about when to say "I don't know".
  - Adjusting how much context is passed.

- **Vibe coding**  
  This project encourages exploration:
  - Swap out the embedding model.
  - Replace `Demystifying The Myth.txt` with another domain (e.g., course handbook, FAQ, docs).
  - Tweak the Streamlit UI to show more or less information.

## Suggested exercises

1. **Change the knowledge base**  
   Replace `Demystifying The Myth.txt` with different documents (e.g., your own project policies, a course handbook, FAQs, or technical docs) and see how the chatbot behaves.

2. **Adjust chunking**  
   In `app.py` and/or `test_chromadb.ipynb`, modify how the text is split into paragraphs (e.g., fixed-length chunks, headings-based splitting) and observe retrieval quality.

3. **Experiment with k**  
   Use the sidebar slider to change the number of retrieved paragraphs. When does more context help? When does it confuse the LLM?

4. **Prompt variations**  
   Edit the prompt inside `generate_answer` and compare answers:
   - More concise vs. more detailed.
   - Different system roles (e.g., "strict policy enforcer" vs. "friendly assistant").

5. **Model experiments (optional)**  
   If you have access to different Gemini models, try switching `LLM_MODEL_NAME` in `app.py` and see how answers differ in style/quality/latency.

## Safety and privacy

- Do **not** commit your `.env` file or real API keys to git or share them publicly.
- The included `.gitignore` helps prevent accidental commits of secrets and local artifacts.

## Summary

This repo gives you a concrete, end-to-end example of:
- Loading and chunking a text corpus
- Building an embedding-based retriever with Chroma
- Calling an LLM (Gemini) with retrieved context
- Exposing the whole flow through a simple Streamlit chatbot

Use it as a starting point to build your own RAG systems, and to experiment with both **vibe coding** and **prompt engineering** in a safe, self-contained environment.
