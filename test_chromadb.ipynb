{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "922c5c82",
   "metadata": {},
   "source": [
    "## importing chromadb library \n",
    "import chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecbce2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "import uuid\n",
    "\n",
    "client = chromadb.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70dc3df1",
   "metadata": {},
   "source": [
    "## creating collection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d402dfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_name = \"policies\"\n",
    "\n",
    "# Check if a collection with this name already exists\n",
    "\"\"\" existing_collections = client.list_collections()\n",
    "existing_names = [c.name for c in existing_collections]\n",
    "\n",
    "if collection_name in existing_names:\n",
    "    collection = client.get_collection(name=collection_name)\n",
    "    print(f\"Collection '{collection_name}' already exists. Using existing collection.\")\n",
    "else:\n",
    "    collection = client.create_collection(name=collection_name)\n",
    "    print(f\"Collection '{collection_name}' created successfully!\") \"\"\"\n",
    "\n",
    "collection = client.get_or_create_collection(name=collection_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "185dfe37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\2025\\VDB\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total paragraphs (chunks) found: 33\n",
      "Policy paragraphs added to Chroma collection.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# 1. Load the full policies text from file\n",
    "with open(\"policies.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    policy_text = f.read()\n",
    "\n",
    "# 2. Split into paragraph-level chunks (separated by blank lines)\n",
    "raw_chunks = [p.strip() for p in policy_text.split(\"\\n\\n\") if p.strip()]\n",
    "print(f\"Total paragraphs (chunks) found: {len(raw_chunks)}\")\n",
    "\n",
    "# 3. Initialize the embedding model (once per notebook)\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# 4. Generate UUID ids for each chunk\n",
    "ids = [str(uuid.uuid4()) for _ in raw_chunks]\n",
    "\n",
    "# 5. Embed each chunk\n",
    "embeddings = model.encode(raw_chunks).tolist()  # list of [dim]-vectors\n",
    "\n",
    "# 6. Add all chunks to the 'policies' collection\n",
    "collection.add(\n",
    "    ids=ids,\n",
    "    documents=raw_chunks,\n",
    "    embeddings=embeddings,\n",
    "    metadatas=[{\"source\": \"policies.txt\", \"type\": \"paragraph\"} for _ in raw_chunks],\n",
    ")\n",
    "\n",
    "print(\"Policy paragraphs added to Chroma collection.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "193687bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result 1\n",
      "Distance: 1.5577661991119385\n",
      "Metadata: {'source': 'policies.txt', 'type': 'paragraph'}\n",
      "Document:\n",
      " Accessibility:\n",
      "- The Store aims to provide an accessible environment for all customers, including individuals with disabilities.\n",
      "- Requests for reasonable accommodations can be made to store management or customer service staff.\n",
      "--------------------------------------------------------------------------------\n",
      "Result 2\n",
      "Distance: 1.7276356220245361\n",
      "Metadata: {'type': 'paragraph', 'source': 'policies.txt'}\n",
      "Document:\n",
      " By shopping at BrightCart Retail or using our services, you acknowledge that you have read, understood, and agree to abide by these policies, as may be amended from time to time.\n",
      "--------------------------------------------------------------------------------\n",
      "Result 3\n",
      "Distance: 1.7711091041564941\n",
      "Metadata: {'source': 'policies.txt', 'type': 'paragraph'}\n",
      "Document:\n",
      " 9. Health, Safety, and Accessibility\n",
      "Health and Safety:\n",
      "- Customers are required to comply with all posted health and safety guidelines (e.g., hygiene practices, occupancy limits).\n",
      "- In the event of a public health emergency, additional measures (such as mask requirements or special shopping hours) may be implemented.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Query the policies collection with a natural language question\n",
    "query_text = \"who is this meant for\"\n",
    "\n",
    "# Embed the query\n",
    "query_embedding = model.encode([query_text]).tolist()\n",
    "\n",
    "# Retrieve the top-k most similar paragraphs from Chroma\n",
    "results = collection.query(\n",
    "    query_embeddings=query_embedding,\n",
    "    n_results=3,\n",
    ")\n",
    "\n",
    "# Pretty-print the results\n",
    "for i, (doc, meta, dist) in enumerate(zip(results[\"documents\"][0], results[\"metadatas\"][0], results[\"distances\"][0])):\n",
    "    print(f\"Result {i+1}\")\n",
    "    print(\"Distance:\", dist)\n",
    "    print(\"Metadata:\", meta)\n",
    "    print(\"Document:\\n\", doc)\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3bbc3a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemini LLM (gemini-2.5-flash) initialized.\n"
     ]
    }
   ],
   "source": [
    "# Configure Gemini LLM using API key from .env\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"GEMINI_API_KEY not set in .env. Please edit .env and set GEMINI_API_KEY=your_actual_key.\")\n",
    "\n",
    "# Configure Gemini client\n",
    "genai.configure(api_key=api_key)\n",
    "\n",
    "# Use the specified Gemini model\n",
    "llm = genai.GenerativeModel(\"gemini-2.5-flash\")\n",
    "print(\"Gemini LLM (gemini-2.5-flash) initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a005fbb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most unopened items in new and resalable condition may be returned within 30 days of purchase with an original receipt.\n"
     ]
    }
   ],
   "source": [
    "# RAG-style helper: retrieve from Chroma, then answer with Gemini\n",
    "\n",
    "def answer_policies_question(question: str, k: int = 3) -> str:\n",
    "    \"\"\"Retrieve top-k policy paragraphs relevant to the question and ask Gemini to answer.\n",
    "\n",
    "    Assumes:\n",
    "    - `collection` is a Chroma collection with policy paragraphs.\n",
    "    - `model` is the SentenceTransformer embedding model.\n",
    "    - `llm` is the configured Gemini GenerativeModel.\n",
    "    \"\"\"\n",
    "    # 1. Embed the question\n",
    "    query_embedding = model.encode([question]).tolist()\n",
    "\n",
    "    # 2. Retrieve top-k paragraphs from Chroma\n",
    "    results = collection.query(\n",
    "        query_embeddings=query_embedding,\n",
    "        n_results=k,\n",
    "    )\n",
    "\n",
    "    retrieved_docs = results[\"documents\"][0]\n",
    "\n",
    "    # 3. Build a context string from the retrieved paragraphs\n",
    "    context = \"\\n\\n\".join(retrieved_docs)\n",
    "\n",
    "    # 4. Construct a prompt for Gemini\n",
    "    prompt = f\"\"\"You are a helpful assistant for a retail store.\n",
    "Use ONLY the information in the CONTEXT section below to answer the QUESTION.\n",
    "If the answer is not clearly present in the context, say that you don't know based on the given policies.\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "Answer clearly and concisely.\n",
    "\"\"\"\n",
    "\n",
    "    response = llm.generate_content(prompt)\n",
    "\n",
    "    # `response.text` is usually the main text answer. Fallback to str(response) if needed.\n",
    "    return getattr(response, \"text\", str(response))\n",
    "\n",
    "\n",
    "# Example usage (you can change the question text and re-run):\n",
    "example_answer = answer_policies_question(\"What is the return policy on unopened items?\", k=3)\n",
    "print(example_answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
